{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "286b16d7-9e36-4c2f-bffd-f98b7dd0fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from langchain.embeddings import HuggingFaceEmbeddings\n",
    "    from langchain.vectorstores import FAISS\n",
    "    from langchain.llms import LlamaCpp\n",
    "    from langchain.chains import LLMChain,RetrievalQA, ConversationalRetrievalChain\n",
    "    import streamlit as st\n",
    "    from langchain.callbacks.manager import CallbackManager\n",
    "    from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "    from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc7129ca-e943-4d4b-88e7-327395e84648",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FAISS_PATH = 'vectorstore/db_faiss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a23fc26a-4e99-47b2-9e6f-faa85d9c2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                       model_kwargs={'device': 'cpu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bced11d3-52fa-4ada-96e3-217fbdfe0197",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.load_local(DB_FAISS_PATH, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7db6f2cb-99f4-41d8-8cbb-a87ae52eaf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93cf4880-15b4-4433-a3b3-c46b6833f17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./models/llama-2-7b-chat.ggmlv3.q4_K_M.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 15 (mostly Q4_K - Medium)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 4289.33 MB (+ 1024.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1024.00 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "        model_path=\"./models/llama-2-7b-chat.ggmlv3.q4_K_M.bin\",\n",
    "        input={\"temperature\": 0.75, \"max_length\": 2000, \"top_p\": 1},\n",
    "        #n_gpu_layers=n_gpu_layers,\n",
    "        #n_batch=n_batch,\n",
    "        n_ctx=2048,\n",
    "        f16_kv=True,\n",
    "        callback_manager=callback_manager,\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef39c22f-125f-4680-981b-ea1f4a28e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "retriever= db.as_retriever(search_kwargs={\"k\": 3, \"search_type\": \"similarity\"})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd63dd41-c49b-4206-a068-9092fdee2196",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(llm=llm, chain_type=\"stuff\", retriever=retriever,return_source_documents = False, verbose = False, memory=memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0840e294-1e62-4e4b-a737-9f12610a22be",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"what are the traits of an explainable AI system?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b62103f-fca3-45a1-aca5-7df5ce6202ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What is the purpose of providing transparency into how an AI system arrived at a particular decision or prediction?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 10262.25 ms\n",
      "llama_print_timings:      sample time =    17.32 ms /    24 runs   (    0.72 ms per token,  1385.28 tokens per second)\n",
      "llama_print_timings: prompt eval time = 375661.71 ms /   298 tokens ( 1260.61 ms per token,     0.79 tokens per second)\n",
      "llama_print_timings:        eval time = 754492.15 ms /    23 runs   (32804.01 ms per token,     0.03 tokens per second)\n",
      "llama_print_timings:       total time = 1130450.62 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The purpose of providing transparency into how an AI system arrived at a particular decision or prediction is to build trust in the model by providing human-understandable rationales for its behavior. This allows humans to judge whether the explanation is justified and to understand the reasoning behind the model's predictions, ultimately leading to increased trust in the model. The purpose of providing transparency into how an AI system arrived at a particular decision or prediction is to build trust in the model by providing human-understandable rationales for its behavior. This allows humans to judge whether the explanation is justified and to understand the reasoning behind the model's predictions, ultimately leading to increased trust in the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 10262.25 ms\n",
      "llama_print_timings:      sample time =    51.28 ms /    73 runs   (    0.70 ms per token,  1423.64 tokens per second)\n",
      "llama_print_timings: prompt eval time = 1475859.21 ms /   597 tokens ( 2472.13 ms per token,     0.40 tokens per second)\n",
      "llama_print_timings:        eval time = 687155.46 ms /    72 runs   ( 9543.83 ms per token,     0.10 tokens per second)\n",
      "llama_print_timings:       total time = 2163777.50 ms\n"
     ]
    }
   ],
   "source": [
    "response = chain(({\"question\":question }))\n",
    "print(response[\"answer\"])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5596777-f258-4257-8fb1-68d99a5a23f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
